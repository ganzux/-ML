{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='steelblue'>\n",
    "<h3>\n",
    "<span style=\"font-family:Comic sans MS; font-size:1.5em;\">\n",
    "Using saved pipeline and model<br>\n",
    "\n",
    "\n",
    " </span>\n",
    "</h3>\n",
    "</font>\n",
    "\n",
    "<font color='gray'>\n",
    "<span style=\"font-family:Comic sans MS; font-size:1.2em;\">\n",
    "Following processing is done:<br>\n",
    "    <ol>\n",
    "        <li><strong>Load New Data:</strong> Load the new dataset on which predictions need to be made</li>\n",
    "        <li><strong>Load pipeline:</strong> Load the pipeline saved during pre-processing of the data</li>\n",
    "        <li><strong>Load Model:</strong> Load the Logistic Regression Model saved during model creation</li>\n",
    "        <li><strong>Transform:</strong> Transform the newly loaded dataframe using the pipeline model</li>\n",
    "        <li><strong>Predictions:</strong> Make predictions using the loaded model on new data</li>\n",
    "        <li><strong>Save Results:</strong> Write the predictions to a csv file</li>\n",
    "    </ol>\n",
    "</span>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment for using pyspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Load Pipeline and Model\")\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"Warn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv').options(header='true', inferSchema='true').load('../datasets/agent-0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the column names so that they can be written to ouput\n",
    "dColumns = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pipeline for data processing<br>\n",
    "<span style=\"font-family:times, serif; font-size:14pt; font-style:bold\">\n",
    "    <ul>\n",
    "<li>Load the saved pipeline into a pipelinemodel.</li>\n",
    "<li>Tranform the dataframe using the pipeline (does all the preprocessing that was done during original processing of the dataframe)</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to provide the folder where the pipeline was stored\n",
    "pipelineModel = PipelineModel.load('projPipeline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new dataframe is created and a features column is created\n",
    "preppedDF = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preppedDF.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Logistic Regression Model<br>\n",
    "<span style=\"font-family:times, serif; font-size:14pt; font-style:bold\">\n",
    "    <ul>\n",
    "        <li>Load the saved logistic regression model</li>\n",
    "        <li>Transform the processed dataframe and make predictions</li>\n",
    "        <li>Prepare a new dataframe with predictions and display it</li>\n",
    "    </ul>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegressionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = LogisticRegressionModel.load('projlrModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(preppedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the features column\n",
    "dColumns.append('prediction')\n",
    "selected = predictions.select(dColumns)\n",
    "selected.limit(10).toPandas().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the results folder if it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "dirpath = './results'\n",
    "try:\n",
    "    shutil.rmtree(dirpath)\n",
    "except OSError as e:\n",
    "    print(\"Error/Info: %s : %s\" % (dirpath, e.strerror))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the predictions to csv file (results folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.write.format('csv')\\\n",
    "              .option('header', True)\\\n",
    "              .save('results')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
