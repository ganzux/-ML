{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Cleaning & Feature Engineering using Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='steelblue'>\n",
    "<h3>\n",
    "<span style=\"font-family:Comic sans MS; font-size:1.5em;\">\n",
    "Features in the dataset (target: survived)<br>\n",
    " </span>\n",
    "</h3>\n",
    "</font>\n",
    "\n",
    "<font color='gray'>\n",
    "<span style=\"font-family:Comic sans MS; font-size:1.4em;\">\n",
    "    <ul>\n",
    "        <li> <strong>Pclass:</strong> Passenger Class  (1 = 1st; 2 = 2nd; 3 = 3rd)</li>\n",
    "        <li> <strong>sex:</strong> Gender </li>\n",
    "        <li> <strong>sibsp:</strong> Number of siblings/Spouses </li>\n",
    "        <li> <strong>parch :</strong> Number of parents/children</li>\n",
    "        <li> <strong>fare:</strong> travel fare</li>\n",
    "        <li> <strong>Embarked:</strong> Boarded in (C = Cherbourg; Q = Queenstown; S = Southampton)</li>\n",
    "        <li> <strong>boat:</strong> Life boat</li>\n",
    "        <li> <strong>body:</strong> Body identification number</li>\n",
    "        <li> <strong>home.dest:</strong> Destination</li>\n",
    "        <li> <strong>ticket:</strong> Ticket number</li>\n",
    "        <li> <strong>cabin:</strong> Cabin number</li>\n",
    "        <li> <strong>name:</strong> Passenger name</li>\n",
    "        <li> <strong>survived:</strong> Did the passenger survive? (0: No, 1: Yes)</li>        \n",
    "    </ul>\n",
    "</span>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment for using pyspark\n",
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Application Context\n",
    "spark = SparkSession.builder.appName(\"Titanic Dataset\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"Warn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset file which is in csv - comma separated values format\n",
    "sdf = spark.read.format('csv').options(header='true', inferSchema='true').load('../datasets/titanic3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = sdf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['boat', 'body', 'home.dest', 'ticket', 'cabin', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop interface on dataframe requires individual string\n",
    "# to pass a list convert the argument as *args\n",
    "sdf = sdf.drop(*to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Shape: ({},{})'. format(sdf.count(), len(sdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the survived has null value in it then drop the sample\n",
    "from pyspark.sql.functions import col\n",
    "sdf = sdf.where(col(\"survived\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape: ({},{})'. format(sdf.count(), len(sdf.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show columns with null values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "def show_null_value_count(df):\n",
    "    null_columns_counts = []\n",
    "    numRows = df.count()\n",
    "    for k in df.columns:\n",
    "        nullRows = df.where(col(k).isNull()).count()\n",
    "        if(nullRows > 0):\n",
    "            temp = k,nullRows\n",
    "            null_columns_counts.append(temp)\n",
    "    spark.createDataFrame(null_columns_counts, ['Column_With_Null_Value', 'Null_Values_Count']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_null_value_count(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the count - indicator of columns with missing values\n",
    "# Check for non numeric values in columns e.g. sex, emabarked, name\n",
    "sdf.describe().toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find distinct values in embarked\n",
    "sdf.select('embarked').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.crosstab('age', 'sex').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupby('age').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change gender to numeric values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol='sex', outputCol=\"sex_numeric\").fit(sdf)\n",
    "sdf = indexer.transform(sdf)\n",
    "sdf.select('sex', 'sex_numeric').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle missing values in age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "# Note: as of Spark 2.4.5 Imputer is still documented as Experimental\n",
    "# strategy  could be mean or median\n",
    "imputer = Imputer(inputCols=['age'], outputCols=['age_filled'], strategy = 'mean')\n",
    "model = imputer.fit(sdf)\n",
    "sdf = model.transform(sdf)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace the missing values in fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(inputCols=['fare'], outputCols=['fare_filled'])\n",
    "model = imputer.fit(sdf)\n",
    "sdf = model.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embarked Feature processing\n",
    "1. replace null values with most occuring\n",
    "2. create indexer to replace the string values\n",
    "3. apply onehotencoderestimator on the indexed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'black'>\n",
    "<h2> Embarked Feature processing</h2><br>\n",
    "<span style=\"font-family:times, serif; font-size:14pt; font-style:bold\">\n",
    "<ol>\n",
    "    <li>replace null values with most occuring</li>\n",
    "    <li>create indexer to replace the string values</li>\n",
    "    <li>apply onehotencoderestimator on the indexed values</li>\n",
    "</ol>\n",
    "</span>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupBy('embarked').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.na.fill({\"embarked\" :'S'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupBy('embarked').count().orderBy('count').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now apply indexer and OneHotEncoding to embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol='embarked', outputCol=\"embarked_numeric\").fit(sdf)\n",
    "sdf = indexer.transform(sdf)\n",
    "sdf.select('embarked', 'embarked_numeric').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"embarked_numeric\"],\n",
    "                                 outputCols=[\"embarked_Vec\"])\n",
    "model = encoder.fit(sdf)\n",
    "sdf = model.transform(sdf)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape: ({},{})'. format(sdf.count(), len(sdf.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "fCols = ['pclass', 'sex_numeric', 'age_filled', 'sibsp', 'parch', 'fare_filled', 'embarked_Vec']\n",
    "feature = VectorAssembler(inputCols = fCols, outputCol = 'features')\n",
    "data = feature.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='teal'>\n",
    "    <h1>Now Data is clean</h1>\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
